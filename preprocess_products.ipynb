{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Python36\\lib\\site-packages\\pandas\\core\\frame.py:3997: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "\n",
    "import plotly.express as px\n",
    "import re\n",
    "\n",
    "\n",
    "# Source Code\n",
    "products_raw = pd.read_csv(\"ProductRaw.csv\",error_bad_lines=False)\n",
    "reviews_raw = pd.read_csv('ReviewRaw.csv',error_bad_lines=False)\n",
    "#--------------\n",
    "\n",
    "# PREPROCESSING\n",
    "\n",
    "# Drop duplicate if data have\n",
    "products = products_raw.drop_duplicates()\n",
    "\n",
    "# Droping duplicate values\n",
    "reviews = reviews_raw.drop_duplicates()\n",
    "\n",
    "# Remove feature 'full_name' and 'created_time'\n",
    "reviews.drop(['full_name','created_time'],axis=1,inplace=True)\n",
    "\n",
    "# Combine products and reviews together\n",
    "reviews = reviews[reviews.product_id.isin(products.item_id)]\n",
    "\n",
    "# Reset index reviews\n",
    "reviews.reset_index(drop=True,inplace=True)\n",
    "\n",
    "# Save to file\n",
    "reviews.to_csv('Review_new.csv')\n",
    "products.to_csv('Product_new.csv')\n",
    "\n",
    "# Load new csv file after preprocessing\n",
    "\n",
    "new_products = pd.read_csv('Product_new.csv',lineterminator='\\n')\n",
    "new_reviews = pd.read_csv('Review_new.csv',lineterminator='\\n')\n",
    "\n",
    "# Exploding the data\n",
    "\n",
    "# pd.options.display.float_format = '{:,.2f}'.format\n",
    "\n",
    "# Brand\n",
    "# brands = new_products.groupby('brand')['item_id'].count().sort_values(ascending=False)\n",
    "\n",
    "# group rating in product's dataset\n",
    "# new_products.groupby(['rating'])['item_id'].count().head(5)\n",
    "\n",
    "# Product rating by customer's review\n",
    "# avg_rating_customer = new_reviews.groupby(by='product_id').mean()['rating'].to_frame().reset_index()\n",
    "# avg_rating_customer.rename({'rating':'avg_rating'},axis=1,inplace=True)\n",
    "\n",
    "# Top 20 customer make review\n",
    "# top_rating_customers = new_reviews.groupby('customer_id').count()['product_id'].sort_values(ascending=False)[:20]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Contentbased solution\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel,cosine_similarity\n",
    "from underthesea import word_tokenize, pos_tag, sent_tokenize\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "new_products = new_products[new_products['name'].notnull()]\n",
    "new_products['name_description'] = new_products['name'] + new_products['description']\n",
    "products = new_products[new_products['name_description'].notnull()]\n",
    "products['name_description_pre'] = products['name_description']\\\n",
    "    .apply(lambda x: word_tokenize(x, format='text'))\n",
    "products = products.reset_index()\n",
    "\n",
    "# Stop words\n",
    "STOP_WORD_FILE = 'vietnamese-stopwords.txt'\n",
    "\n",
    "with open(STOP_WORD_FILE,'r',encoding='utf-8') as file:\n",
    "    stop_words = file.read()\n",
    "\n",
    "stop_words = stop_words.split('\\n')\n",
    "\n",
    "# TF-IDF\n",
    "tf = TfidfVectorizer(analyzer='word',min_df=0,stop_words=stop_words)\n",
    "\n",
    "tfidf_matrix = tf.fit_transform(products.name_description_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-14 09:49:26.061 INFO    gensim.corpora.dictionary: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-12-14 09:49:27.805 INFO    gensim.corpora.dictionary: built Dictionary(36940 unique tokens: ['airpod', 'apple', 'bao_gồm', 'bluetooth', 'bluetooth_inpods']...) from 4370 documents (total 1529910 corpus positions)\n",
      "2021-12-14 09:49:27.806 INFO    gensim.utils: Dictionary lifecycle event {'msg': \"built Dictionary(36940 unique tokens: ['airpod', 'apple', 'bao_gồm', 'bluetooth', 'bluetooth_inpods']...) from 4370 documents (total 1529910 corpus positions)\", 'datetime': '2021-12-14T09:49:27.806288', 'gensim': '4.1.2', 'python': '3.6.8 (tags/v3.6.8:3c6b436a57, Dec 24 2018, 00:16:47) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'created'}\n",
      "2021-12-14 09:49:28.828 INFO    gensim.models.tfidfmodel: collecting document frequencies\n",
      "2021-12-14 09:49:28.829 INFO    gensim.models.tfidfmodel: PROGRESS: processing document #0\n",
      "2021-12-14 09:49:29.165 INFO    gensim.utils: TfidfModel lifecycle event {'msg': 'calculated IDF weights for 4370 documents and 36940 features (827250 matrix non-zeros)', 'datetime': '2021-12-14T09:49:29.165886', 'gensim': '4.1.2', 'python': '3.6.8 (tags/v3.6.8:3c6b436a57, Dec 24 2018, 00:16:47) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'initialize'}\n",
      "2021-12-14 09:49:29.166 INFO    gensim.similarities.docsim: creating sparse index\n",
      "2021-12-14 09:49:29.167 INFO    gensim.matutils: creating sparse matrix from corpus\n",
      "2021-12-14 09:49:29.169 INFO    gensim.matutils: PROGRESS: at document #0\n",
      "2021-12-14 09:49:34.825 INFO    gensim.similarities.docsim: created <4370x36940 sparse matrix of type '<class 'numpy.float32'>'\n",
      "\twith 827250 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Solution 2: Gensim\n",
    "\n",
    "from gensim import corpora, models, similarities\n",
    "import jieba\n",
    "import re\n",
    "\n",
    "# Preprocess\n",
    "\n",
    "products['name_description_pre'].replace([\"[,-/–]\",\"[().\\d]\",\"^\\['|'\\]$\"],\"\",regex=True,inplace=True)\n",
    "\n",
    "# Tokenize(split) the sentences into words\n",
    "intro_products = [[text for text in x.split()] for x in products.name_description_pre]\n",
    "\n",
    "# Remove ' ' elements in text\n",
    "intro_products_re = [[t.lower() for t in text if not t in [\" \",\"±\",\"\",\"?\",\":\",\"…\",\"•\",\"[\",\"]\",\"...\",\"≥\",\"%\",\"”\",\"“\",\"&\",\"*\",\"\\ |\\?|\\.|\\!|\\/|\\;|\\:\"]] for text in intro_products]\n",
    "\n",
    "# Obtain the number of features based on dictionary: Use corpora.Dictionary\n",
    "dictionary = corpora.Dictionary(intro_products_re)\n",
    "\n",
    "# List of features in dictionary\n",
    "dic_token_2id = dictionary.token2id\n",
    "\n",
    "# Number of features (word) in dictionary\n",
    "feature_cnt = len(dic_token_2id)\n",
    "\n",
    "# Obtain corpus based on dictionary (dense matrix)\n",
    "corpus = [dictionary.doc2bow(text) for text in intro_products_re]\n",
    "# Use TF-IDF model to process corpus, obtaining index\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "# Couting similarity in sparse matrix\n",
    "index = similarities.SparseMatrixSimilarity(tfidf[corpus], num_features=feature_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "products.to_csv('final_product.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file_name_dictionary = 'Dictionary.sav'\n",
    "pickle.dump(dictionary, open(file_name_dictionary, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_tfidf = 'TfidfModel.sav'\n",
    "pickle.dump(tfidf, open(file_name_tfidf, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_index = 'Index.sav'\n",
    "pickle.dump(index, open(file_name_index, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f7291e4b392a32fbfa525b87d1bbd0a3d888adf3d0deca0c205c61b9e7284b82"
  },
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
